{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 4. Writing Structured Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a list of words = ['is', 'it', 'good', '?']. a) Use a series of assignment statements (e.g. words[1] = words[2]) and a temporary variable tmp to transform this list into the list ['it', 'is', 'good', '!']. b) Now do the same transformation using tuple assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['is', 'it', 'good', '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'is', 'good', '!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  a) Use a series of assignment statements \n",
    "tmp = sample[0]\n",
    "sample[0] = sample[1]\n",
    "sample[1] = tmp\n",
    "sample[3] = '!'\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# b) Now do the same transformation using tuple assignment.\n",
    "sample = ['is', 'it', 'good', '?']\n",
    "sampe_t = ['!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[3] = sampe_t[0]\n",
    "\n",
    "sample[0], sample[1] = sample[1], sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'is', 'good', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Write code that removes whitespace at the beginning and end of a string ('   this   is   a    sample   sentence   '), and normalizes whitespace between words to be a single space character.\n",
    "#### a) do this task using split() and join()\n",
    "#### b) do this task using regular expression substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ' this is a sample sentence '\n",
    "string2 = 'this is a sample sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) do this task using split() and join()\n",
    "\n",
    "def white_space(string):\n",
    "    x = string.split()\n",
    "    x = ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a sample sentence'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_space(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a sample sentence'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b) do this task using regular expression substitutions\n",
    "\n",
    "import nltk\n",
    "string = ' this is a sample sentence '\n",
    "def white_space2(string):\n",
    "    x = re.sub(\"^\\s\",'',string)\n",
    "    x = re.sub(\"\\s$\",'',x)\n",
    "    x = re.sub(\"\\s\\s+\", ' ',x)\n",
    "    return x\n",
    "white_space2(string)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. sent1=['The', 'dog', 'gave', 'John', 'the', 'newspaper']. Now assign sent2=sent1. Modify sent1[1]='monkey'. Please review section 4.1 -Assignment in Chapter 4 to answer the following questions:\n",
    "#### a) verify that sent2 has changed\n",
    "#### b) Now try the same exercise but instead assign sent2=sent1[:]. Modify sent1[1]='monkey' and see what happens to sent2. Explain.\n",
    "#### c) Now define text1=[['The', 'dog', 'gave', 'John', 'the', 'newspaper'], ['John', 'is', 'happy']]. Now assign text2=text1[:], assign a new value to one of the words (text1[0][1]='monkey'). Check what happens to text2. Explain.\n",
    "#### d) Extract successive overlapping 4-grams from ['The', 'dog', 'gave', 'John', 'the', 'newspaper']. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) verify that sent2 has changed\n",
    "\n",
    "sent1=['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "sent2=sent1\n",
    "sent1[1]='monkey'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'monkey', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The value has changed\n",
    "\n",
    "sent2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Now try the same exercise but instead assign sent2=sent1[:]. Modify sent1[1]='monkey' and see what happens to sent2. Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1=['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "sent2=sent1[:]\n",
    "sent1[1]='monkey'\n",
    "sent2\n",
    "\n",
    "# This methods assign entire sent1 as sents2 at the moment and it doesn't change dog to monkey\n",
    "# where as the first method only assign second material from sent1 and it change dog to monkey\n",
    "# The difference is [:] makes A shallow copy which creates a new object which stores the reference of the original elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Now define text1=[['The', 'dog', 'gave', 'John', 'the', 'newspaper'], ['John', 'is', 'happy']]. Now assign text2=text1[:], assign a new value to one of the words (text1[0][1]='monkey'). Check what happens to text2. Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=[['The', 'dog', 'gave', 'John', 'the', 'newspaper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=text1[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1[0][1]='monkey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'monkey', 'gave', 'John', 'the', 'newspaper']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'monkey', 'gave', 'John', 'the', 'newspaper']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method doesn't create a shallow copy and directly use it from text 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Extract successive overlapping 4-grams from ['The', 'dog', 'gave', 'John', 'the', 'newspaper']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'dog', 'gave', 'John'],\n",
       " ['dog', 'gave', 'John', 'the'],\n",
       " ['gave', 'John', 'the', 'newspaper']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent[i:i+n] for i in range(len(sent)-n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Write a function that prints any word that appeared in the last 20% of a text that had not been encountered earlier. Use text1 from nltk.book to call this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted((w.lower() for w in text1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords    \n",
    "\n",
    "def hi(text):\n",
    "    length = len(sorted((w.lower() for w in text)))\n",
    "    threshold = round(length*0.2)\n",
    "    text_up = text[:threshold]\n",
    "    threshold += 1\n",
    "    text_down = text[threshold:]\n",
    "    text_top = sorted(w for w in set(text_up) if w.isalpha())\n",
    "    text_bottom = sorted(w for w in set(text_down) if w.isalpha())\n",
    "    for element in text_bottom:\n",
    "        if element in text_top:\n",
    "            text_bottom.remove(element)\n",
    "    print(random.choice(text_bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goose\n"
     ]
    }
   ],
   "source": [
    "hi(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irregular\n"
     ]
    }
   ],
   "source": [
    "hi(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Write a program that takes the sentence (\"we have seen two kinds of two sequence objects\") expressed as a single string, splits it and counts up the tokens. Get it to print out each token and the token's frequency, one per line, in alphabetical order. You should write a function and call that function to process the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"we have seen two kinds of two sequence objects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def token(sent):\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "    string = str(sent)\n",
    "    split = string.split()\n",
    "    length = len(split)\n",
    "    print(f\"number of tokens is {length}\")\n",
    "    freq_table = {}\n",
    "    for word in split:\n",
    "        if word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "    dict1 = OrderedDict(sorted(freq_table.items()))\n",
    "    for k, v in dict1.items():\n",
    "        print(k, v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens is 9\n",
      "have 1\n",
      "kinds 1\n",
      "objects 1\n",
      "of 1\n",
      "seen 1\n",
      "sequence 1\n",
      "two 2\n",
      "we 1\n"
     ]
    }
   ],
   "source": [
    "token(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Write a function shorten(word, n) to process a text (“big big big world today tomorrow good Today good”), omitting the n most frequently occurring words of the text. You should use w.lower() to normalize the text first. Please call this shorten function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten(word,n):\n",
    "    words = word.split()\n",
    "    words = [word.lower() for word in words]\n",
    "    ngram = [words[i:i+n] for i in range(len(words)-n+1)]\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big', 'big', 'big'],\n",
       " ['big', 'big', 'world'],\n",
       " ['big', 'world', 'today'],\n",
       " ['world', 'today', 'tomorrow'],\n",
       " ['today', 'tomorrow', 'good'],\n",
       " ['tomorrow', 'good', 'today'],\n",
       " ['good', 'today', 'good']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"big big big world today tomorrow good Today good\"\n",
    "\n",
    "shorten(word,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Please use the sample code from Lab 4 about the TF-IDF to summarize your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The current tax environment is characterized by tax authorities requiring increased transparency across jurisdictions, sharing information with each other and applying determined approaches to the tax management and collection process, but with a lack of consistency across borders. This is expanding the compliance workload for businesses, while they are exposed to increased tax risk and uncertainty about the sustainability of current business models and group structures. To help overcome these challenges, businesses are centralizing compliance, using technology to aggregate, validate and report for compliance purposes, and using data analytics on the information they have gathered to identify anomalies and mitigate risk. To manage this changing landscape, alongside the increased use of analytics, tax authorities and tax advisors are starting to explore the possibilities for deploying sophisticated data analytics and Artificial Intelligence (AI) in tax to facilitate compliance and assist professionals and their clients with commonly encountered questions. While data analytics has received a lot of attention, Artificial Intelligence in tax is a relatively new phenomenon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text) # NLTK function\n",
    "total_documents = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The current tax': {'current': 1, 'tax': 3, 'environ': 1, 'character': 1, 'author': 1, 'requir': 1, 'increas': 1, 'transpar': 1, 'across': 2, 'jurisdict': 1, ',': 2, 'share': 1, 'inform': 1, 'appli': 1, 'determin': 1, 'approach': 1, 'manag': 1, 'collect': 1, 'process': 1, 'lack': 1, 'consist': 1, 'border': 1, '.': 1}, 'This is expandi': {'thi': 1, 'expand': 1, 'complianc': 1, 'workload': 1, 'busi': 2, ',': 1, 'expos': 1, 'increas': 1, 'tax': 1, 'risk': 1, 'uncertainti': 1, 'sustain': 1, 'current': 1, 'model': 1, 'group': 1, 'structur': 1, '.': 1}, 'To help overcom': {'help': 1, 'overcom': 1, 'challeng': 1, ',': 4, 'busi': 1, 'central': 1, 'complianc': 2, 'use': 2, 'technolog': 1, 'aggreg': 1, 'valid': 1, 'report': 1, 'purpos': 1, 'data': 1, 'analyt': 1, 'inform': 1, 'gather': 1, 'identifi': 1, 'anomali': 1, 'mitig': 1, 'risk': 1, '.': 1}, 'To manage this ': {'manag': 1, 'thi': 1, 'chang': 1, 'landscap': 1, ',': 2, 'alongsid': 1, 'increas': 1, 'use': 1, 'analyt': 2, 'tax': 3, 'author': 1, 'advisor': 1, 'start': 1, 'explor': 1, 'possibl': 1, 'deploy': 1, 'sophist': 1, 'data': 1, 'artifici': 1, 'intellig': 1, '(': 1, 'ai': 1, ')': 1, 'facilit': 1, 'complianc': 1, 'assist': 1, 'profession': 1, 'client': 1, 'commonli': 1, 'encount': 1, 'question': 1, '.': 1}, 'While data anal': {'data': 1, 'analyt': 1, 'ha': 1, 'receiv': 1, 'lot': 1, 'attent': 1, ',': 1, 'artifici': 1, 'intellig': 1, 'tax': 1, 'rel': 1, 'new': 1, 'phenomenon': 1, '.': 1}}\n"
     ]
    }
   ],
   "source": [
    "freq_matrix = _create_frequency_matrix(sentences)\n",
    "print(freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The current tax': {'current': 0.043478260869565216, 'tax': 0.13043478260869565, 'environ': 0.043478260869565216, 'character': 0.043478260869565216, 'author': 0.043478260869565216, 'requir': 0.043478260869565216, 'increas': 0.043478260869565216, 'transpar': 0.043478260869565216, 'across': 0.08695652173913043, 'jurisdict': 0.043478260869565216, ',': 0.08695652173913043, 'share': 0.043478260869565216, 'inform': 0.043478260869565216, 'appli': 0.043478260869565216, 'determin': 0.043478260869565216, 'approach': 0.043478260869565216, 'manag': 0.043478260869565216, 'collect': 0.043478260869565216, 'process': 0.043478260869565216, 'lack': 0.043478260869565216, 'consist': 0.043478260869565216, 'border': 0.043478260869565216, '.': 0.043478260869565216}, 'This is expandi': {'thi': 0.058823529411764705, 'expand': 0.058823529411764705, 'complianc': 0.058823529411764705, 'workload': 0.058823529411764705, 'busi': 0.11764705882352941, ',': 0.058823529411764705, 'expos': 0.058823529411764705, 'increas': 0.058823529411764705, 'tax': 0.058823529411764705, 'risk': 0.058823529411764705, 'uncertainti': 0.058823529411764705, 'sustain': 0.058823529411764705, 'current': 0.058823529411764705, 'model': 0.058823529411764705, 'group': 0.058823529411764705, 'structur': 0.058823529411764705, '.': 0.058823529411764705}, 'To help overcom': {'help': 0.045454545454545456, 'overcom': 0.045454545454545456, 'challeng': 0.045454545454545456, ',': 0.18181818181818182, 'busi': 0.045454545454545456, 'central': 0.045454545454545456, 'complianc': 0.09090909090909091, 'use': 0.09090909090909091, 'technolog': 0.045454545454545456, 'aggreg': 0.045454545454545456, 'valid': 0.045454545454545456, 'report': 0.045454545454545456, 'purpos': 0.045454545454545456, 'data': 0.045454545454545456, 'analyt': 0.045454545454545456, 'inform': 0.045454545454545456, 'gather': 0.045454545454545456, 'identifi': 0.045454545454545456, 'anomali': 0.045454545454545456, 'mitig': 0.045454545454545456, 'risk': 0.045454545454545456, '.': 0.045454545454545456}, 'To manage this ': {'manag': 0.03125, 'thi': 0.03125, 'chang': 0.03125, 'landscap': 0.03125, ',': 0.0625, 'alongsid': 0.03125, 'increas': 0.03125, 'use': 0.03125, 'analyt': 0.0625, 'tax': 0.09375, 'author': 0.03125, 'advisor': 0.03125, 'start': 0.03125, 'explor': 0.03125, 'possibl': 0.03125, 'deploy': 0.03125, 'sophist': 0.03125, 'data': 0.03125, 'artifici': 0.03125, 'intellig': 0.03125, '(': 0.03125, 'ai': 0.03125, ')': 0.03125, 'facilit': 0.03125, 'complianc': 0.03125, 'assist': 0.03125, 'profession': 0.03125, 'client': 0.03125, 'commonli': 0.03125, 'encount': 0.03125, 'question': 0.03125, '.': 0.03125}, 'While data anal': {'data': 0.07142857142857142, 'analyt': 0.07142857142857142, 'ha': 0.07142857142857142, 'receiv': 0.07142857142857142, 'lot': 0.07142857142857142, 'attent': 0.07142857142857142, ',': 0.07142857142857142, 'artifici': 0.07142857142857142, 'intellig': 0.07142857142857142, 'tax': 0.07142857142857142, 'rel': 0.07142857142857142, 'new': 0.07142857142857142, 'phenomenon': 0.07142857142857142, '.': 0.07142857142857142}}\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'current': 2, 'tax': 4, 'environ': 1, 'character': 1, 'author': 2, 'requir': 1, 'increas': 3, 'transpar': 1, 'across': 1, 'jurisdict': 1, ',': 5, 'share': 1, 'inform': 2, 'appli': 1, 'determin': 1, 'approach': 1, 'manag': 2, 'collect': 1, 'process': 1, 'lack': 1, 'consist': 1, 'border': 1, '.': 5, 'thi': 2, 'expand': 1, 'complianc': 3, 'workload': 1, 'busi': 2, 'expos': 1, 'risk': 2, 'uncertainti': 1, 'sustain': 1, 'model': 1, 'group': 1, 'structur': 1, 'help': 1, 'overcom': 1, 'challeng': 1, 'central': 1, 'use': 2, 'technolog': 1, 'aggreg': 1, 'valid': 1, 'report': 1, 'purpos': 1, 'data': 3, 'analyt': 3, 'gather': 1, 'identifi': 1, 'anomali': 1, 'mitig': 1, 'chang': 1, 'landscap': 1, 'alongsid': 1, 'advisor': 1, 'start': 1, 'explor': 1, 'possibl': 1, 'deploy': 1, 'sophist': 1, 'artifici': 2, 'intellig': 2, '(': 1, 'ai': 1, ')': 1, 'facilit': 1, 'assist': 1, 'profession': 1, 'client': 1, 'commonli': 1, 'encount': 1, 'question': 1, 'ha': 1, 'receiv': 1, 'lot': 1, 'attent': 1, 'rel': 1, 'new': 1, 'phenomenon': 1}\n"
     ]
    }
   ],
   "source": [
    "count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "print(count_doc_per_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The current tax': {'current': 0.3979400086720376, 'tax': 0.09691001300805642, 'environ': 0.6989700043360189, 'character': 0.6989700043360189, 'author': 0.3979400086720376, 'requir': 0.6989700043360189, 'increas': 0.2218487496163564, 'transpar': 0.6989700043360189, 'across': 0.6989700043360189, 'jurisdict': 0.6989700043360189, ',': 0.0, 'share': 0.6989700043360189, 'inform': 0.3979400086720376, 'appli': 0.6989700043360189, 'determin': 0.6989700043360189, 'approach': 0.6989700043360189, 'manag': 0.3979400086720376, 'collect': 0.6989700043360189, 'process': 0.6989700043360189, 'lack': 0.6989700043360189, 'consist': 0.6989700043360189, 'border': 0.6989700043360189, '.': 0.0}, 'This is expandi': {'thi': 0.3979400086720376, 'expand': 0.6989700043360189, 'complianc': 0.2218487496163564, 'workload': 0.6989700043360189, 'busi': 0.3979400086720376, ',': 0.0, 'expos': 0.6989700043360189, 'increas': 0.2218487496163564, 'tax': 0.09691001300805642, 'risk': 0.3979400086720376, 'uncertainti': 0.6989700043360189, 'sustain': 0.6989700043360189, 'current': 0.3979400086720376, 'model': 0.6989700043360189, 'group': 0.6989700043360189, 'structur': 0.6989700043360189, '.': 0.0}, 'To help overcom': {'help': 0.6989700043360189, 'overcom': 0.6989700043360189, 'challeng': 0.6989700043360189, ',': 0.0, 'busi': 0.3979400086720376, 'central': 0.6989700043360189, 'complianc': 0.2218487496163564, 'use': 0.3979400086720376, 'technolog': 0.6989700043360189, 'aggreg': 0.6989700043360189, 'valid': 0.6989700043360189, 'report': 0.6989700043360189, 'purpos': 0.6989700043360189, 'data': 0.2218487496163564, 'analyt': 0.2218487496163564, 'inform': 0.3979400086720376, 'gather': 0.6989700043360189, 'identifi': 0.6989700043360189, 'anomali': 0.6989700043360189, 'mitig': 0.6989700043360189, 'risk': 0.3979400086720376, '.': 0.0}, 'To manage this ': {'manag': 0.3979400086720376, 'thi': 0.3979400086720376, 'chang': 0.6989700043360189, 'landscap': 0.6989700043360189, ',': 0.0, 'alongsid': 0.6989700043360189, 'increas': 0.2218487496163564, 'use': 0.3979400086720376, 'analyt': 0.2218487496163564, 'tax': 0.09691001300805642, 'author': 0.3979400086720376, 'advisor': 0.6989700043360189, 'start': 0.6989700043360189, 'explor': 0.6989700043360189, 'possibl': 0.6989700043360189, 'deploy': 0.6989700043360189, 'sophist': 0.6989700043360189, 'data': 0.2218487496163564, 'artifici': 0.3979400086720376, 'intellig': 0.3979400086720376, '(': 0.6989700043360189, 'ai': 0.6989700043360189, ')': 0.6989700043360189, 'facilit': 0.6989700043360189, 'complianc': 0.2218487496163564, 'assist': 0.6989700043360189, 'profession': 0.6989700043360189, 'client': 0.6989700043360189, 'commonli': 0.6989700043360189, 'encount': 0.6989700043360189, 'question': 0.6989700043360189, '.': 0.0}, 'While data anal': {'data': 0.2218487496163564, 'analyt': 0.2218487496163564, 'ha': 0.6989700043360189, 'receiv': 0.6989700043360189, 'lot': 0.6989700043360189, 'attent': 0.6989700043360189, ',': 0.0, 'artifici': 0.3979400086720376, 'intellig': 0.3979400086720376, 'tax': 0.09691001300805642, 'rel': 0.6989700043360189, 'new': 0.6989700043360189, 'phenomenon': 0.6989700043360189, '.': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "print(idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The current tax': {'current': 0.017301739507479895, 'tax': 0.012640436479311706, 'environ': 0.03039000018852256, 'character': 0.03039000018852256, 'author': 0.017301739507479895, 'requir': 0.03039000018852256, 'increas': 0.009645597809406799, 'transpar': 0.03039000018852256, 'across': 0.06078000037704512, 'jurisdict': 0.03039000018852256, ',': 0.0, 'share': 0.03039000018852256, 'inform': 0.017301739507479895, 'appli': 0.03039000018852256, 'determin': 0.03039000018852256, 'approach': 0.03039000018852256, 'manag': 0.017301739507479895, 'collect': 0.03039000018852256, 'process': 0.03039000018852256, 'lack': 0.03039000018852256, 'consist': 0.03039000018852256, 'border': 0.03039000018852256, '.': 0.0}, 'This is expandi': {'thi': 0.023408235804237506, 'expand': 0.04111588260800111, 'complianc': 0.013049926448020964, 'workload': 0.04111588260800111, 'busi': 0.04681647160847501, ',': 0.0, 'expos': 0.04111588260800111, 'increas': 0.013049926448020964, 'tax': 0.005700589000473907, 'risk': 0.023408235804237506, 'uncertainti': 0.04111588260800111, 'sustain': 0.04111588260800111, 'current': 0.023408235804237506, 'model': 0.04111588260800111, 'group': 0.04111588260800111, 'structur': 0.04111588260800111, '.': 0.0}, 'To help overcom': {'help': 0.031771363833455406, 'overcom': 0.031771363833455406, 'challeng': 0.031771363833455406, ',': 0.0, 'busi': 0.018088182212365345, 'central': 0.031771363833455406, 'complianc': 0.02016806814694149, 'use': 0.03617636442473069, 'technolog': 0.031771363833455406, 'aggreg': 0.031771363833455406, 'valid': 0.031771363833455406, 'report': 0.031771363833455406, 'purpos': 0.031771363833455406, 'data': 0.010084034073470746, 'analyt': 0.010084034073470746, 'inform': 0.018088182212365345, 'gather': 0.031771363833455406, 'identifi': 0.031771363833455406, 'anomali': 0.031771363833455406, 'mitig': 0.031771363833455406, 'risk': 0.018088182212365345, '.': 0.0}, 'To manage this ': {'manag': 0.012435625271001175, 'thi': 0.012435625271001175, 'chang': 0.02184281263550059, 'landscap': 0.02184281263550059, ',': 0.0, 'alongsid': 0.02184281263550059, 'increas': 0.006932773425511137, 'use': 0.012435625271001175, 'analyt': 0.013865546851022275, 'tax': 0.00908531371950529, 'author': 0.012435625271001175, 'advisor': 0.02184281263550059, 'start': 0.02184281263550059, 'explor': 0.02184281263550059, 'possibl': 0.02184281263550059, 'deploy': 0.02184281263550059, 'sophist': 0.02184281263550059, 'data': 0.006932773425511137, 'artifici': 0.012435625271001175, 'intellig': 0.012435625271001175, '(': 0.02184281263550059, 'ai': 0.02184281263550059, ')': 0.02184281263550059, 'facilit': 0.02184281263550059, 'complianc': 0.006932773425511137, 'assist': 0.02184281263550059, 'profession': 0.02184281263550059, 'client': 0.02184281263550059, 'commonli': 0.02184281263550059, 'encount': 0.02184281263550059, 'question': 0.02184281263550059, '.': 0.0}, 'While data anal': {'data': 0.01584633925831117, 'analyt': 0.01584633925831117, 'ha': 0.0499264288811442, 'receiv': 0.0499264288811442, 'lot': 0.0499264288811442, 'attent': 0.0499264288811442, ',': 0.0, 'artifici': 0.02842428633371697, 'intellig': 0.02842428633371697, 'tax': 0.006922143786289744, 'rel': 0.0499264288811442, 'new': 0.0499264288811442, 'phenomenon': 0.0499264288811442, '.': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_sentences(tf_idf_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    score a sentence by its word's TF\n",
    "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    sentenceValue = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentenceValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The current tax': 0.02511882588413039, 'This is expandi': 0.028104040104806598, 'To help overcom': 0.02471839896321045, 'To manage this ': 0.01666801164211185, 'While data anal': 0.031782028367025386}\n"
     ]
    }
   ],
   "source": [
    "sentence_scores = _score_sentences(tf_idf_matrix)\n",
    "print(sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_average_score(sentenceValue) -> int:\n",
    "    \"\"\"\n",
    "    Find the average score from the sentence value dictionary\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025278260992256933\n"
     ]
    }
   ],
   "source": [
    "threshold = _find_average_score(sentence_scores)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is expanding the compliance workload for businesses, while they are exposed to increased tax risk and uncertainty about the sustainability of current business models and group structures. While data analytics has received a lot of attention, Artificial Intelligence in tax is a relatively new phenomenon.\n"
     ]
    }
   ],
   "source": [
    "summary=_generate_summary(sentences, sentence_scores, threshold)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.Write a function that takes a list of words (containing duplicates) (i.e. words=['table','chair','desk','table','table','chair']) and returns a list of words (with no duplicates) sorted by decreasing frequency. E.g. if the input list contained 10 instances of the word table and 9 instances of the word chair, then table would appear before chair in the output list. You should use lambda in the sorted( ) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def frequency_sort(sample):\n",
    "    freq_table = {}\n",
    "    for word in sample:\n",
    "        if word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "    sorted_dict = dict( sorted(freq_table.items(),\n",
    "                           key=lambda item: item[1],\n",
    "                           reverse=True))\n",
    "    keysList = list(sorted_dict.keys())\n",
    "    print(keysList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['table' , 'chair' , 'desk' , 'table' , 'table' , 'chair' , 'James' , 'James' , 'James' , 'Lee' , 'Flow' , 'Sick' , 'Youtube' , 'Youtube' , 'Sick' , 'Youtube' , 'You']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['table', 'James', 'Youtube', 'chair', 'Sick', 'desk', 'Lee', 'Flow', 'You']\n"
     ]
    }
   ],
   "source": [
    "frequency_sort(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_sort2(sample):\n",
    "    freq_table = {}\n",
    "    for word in sample:\n",
    "        if word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "    sorted_dict = dict( sorted(freq_table.items(),\n",
    "                           key=lambda item: item[1],\n",
    "                           reverse=True))\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table': 3,\n",
       " 'James': 3,\n",
       " 'Youtube': 3,\n",
       " 'chair': 2,\n",
       " 'Sick': 2,\n",
       " 'desk': 1,\n",
       " 'Lee': 1,\n",
       " 'Flow': 1,\n",
       " 'You': 1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_sort2(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Write a function that takes a text (e.g. text3 from nltk.book) and a vocabulary (e.g. nltk.corpus.words.words()) as its arguments and returns the set of words that appear in the text but not in the vocabulary. Both arguments can be represented as lists of strings. Can you do this by using set.difference()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_of_words(text, vocabulary):\n",
    "    text = set(w.lower() for w in text3 if w.isalpha())\n",
    "    vocabulary = set(w.lower() for w in vocabulary)\n",
    "    \n",
    "    return text.difference(vocabulary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: The Book of Genesis>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist=[w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interpretations',\n",
       " 'morter',\n",
       " 'males',\n",
       " 'jokshan',\n",
       " 'elbethel',\n",
       " 'purchased',\n",
       " 'zoar',\n",
       " 'pison',\n",
       " 'penuel',\n",
       " 'pathrusim',\n",
       " 'failed',\n",
       " 'beguiled',\n",
       " 'sarai',\n",
       " 'handmaids',\n",
       " 'having',\n",
       " 'bereaved',\n",
       " 'peleg',\n",
       " 'sawest',\n",
       " 'magnified',\n",
       " 'merchantmen',\n",
       " 'lieth',\n",
       " 'mourned',\n",
       " 'plains',\n",
       " 'uz',\n",
       " 'ashkenaz',\n",
       " 'hadad',\n",
       " 'hiddekel',\n",
       " 'citi',\n",
       " 'garmen',\n",
       " 'lud',\n",
       " 'daughters',\n",
       " 'friends',\n",
       " 'spi',\n",
       " 'biteth',\n",
       " 'troubled',\n",
       " 'feebler',\n",
       " 'ir',\n",
       " 'doth',\n",
       " 'offered',\n",
       " 'herdmen',\n",
       " 'canaanites',\n",
       " 'magog',\n",
       " 'mahanaim',\n",
       " 'abr',\n",
       " 'sinners',\n",
       " 'shem',\n",
       " 'generations',\n",
       " 'moved',\n",
       " 'twel',\n",
       " 'selfwill',\n",
       " 'fle',\n",
       " 'kiriathaim',\n",
       " 'lehabim',\n",
       " 'gathered',\n",
       " 'kadesh',\n",
       " 'wells',\n",
       " 'gihon',\n",
       " 'countries',\n",
       " 'sevens',\n",
       " 'japheth',\n",
       " 'aram',\n",
       " 'names',\n",
       " 'physicians',\n",
       " 'boug',\n",
       " 'tithes',\n",
       " 'terah',\n",
       " 'rehoboth',\n",
       " 'shelah',\n",
       " 'esek',\n",
       " 'remained',\n",
       " 'kn',\n",
       " 'kadmonites',\n",
       " 'twins',\n",
       " 'poured',\n",
       " 'zaphnathpaaneah',\n",
       " 'tema',\n",
       " 'reumah',\n",
       " 'zilpah',\n",
       " 'kenites',\n",
       " 'ewes',\n",
       " 'shoelatchet',\n",
       " 'ithran',\n",
       " 'wrestlings',\n",
       " 'heav',\n",
       " 'ebal',\n",
       " 'grisled',\n",
       " 'machpelah',\n",
       " 'vestures',\n",
       " 'mesha',\n",
       " 'standest',\n",
       " 'reuben',\n",
       " 'changes',\n",
       " 'bou',\n",
       " 'barr',\n",
       " 'allonbachuth',\n",
       " 'shoulders',\n",
       " 'mocking',\n",
       " 'proceedeth',\n",
       " 'journeyed',\n",
       " 'clo',\n",
       " 'merari',\n",
       " 'assyr',\n",
       " 'jachin',\n",
       " 'grapes',\n",
       " 'beor',\n",
       " 'appointed',\n",
       " 'kittim',\n",
       " 'tru',\n",
       " 'lives',\n",
       " 'eleloheisrael',\n",
       " 'songs',\n",
       " 'souls',\n",
       " 'talked',\n",
       " 'burdens',\n",
       " 'damascus',\n",
       " 'enmishpat',\n",
       " 'sle',\n",
       " 'beriah',\n",
       " 'wrestled',\n",
       " 'cities',\n",
       " 'naamah',\n",
       " 'chesed',\n",
       " 'sowed',\n",
       " 'journeys',\n",
       " 'hebrew',\n",
       " 'perizzites',\n",
       " 'tents',\n",
       " 'hamul',\n",
       " 'served',\n",
       " 'spices',\n",
       " 'camest',\n",
       " 'colours',\n",
       " 'eatest',\n",
       " 'measures',\n",
       " 'reproa',\n",
       " 'garments',\n",
       " 'emins',\n",
       " 'communing',\n",
       " 'bodies',\n",
       " 'stooped',\n",
       " 'abimelech',\n",
       " 'magdiel',\n",
       " 'needeth',\n",
       " 'tarshish',\n",
       " 'pleaseth',\n",
       " 'increased',\n",
       " 'ri',\n",
       " 'chesnut',\n",
       " 'rooms',\n",
       " 'kedar',\n",
       " 'sodom',\n",
       " 'amalek',\n",
       " 'havilah',\n",
       " 'dukes',\n",
       " 'entreated',\n",
       " 'pleased',\n",
       " 'whales',\n",
       " 'kindled',\n",
       " 'timnath',\n",
       " 'saidst',\n",
       " 'firstborn',\n",
       " 'canaanite',\n",
       " 'badest',\n",
       " 'kissed',\n",
       " 'alvah',\n",
       " 'phara',\n",
       " 'sabtah',\n",
       " 'meanest',\n",
       " 'longeth',\n",
       " 'placed',\n",
       " 'fathe',\n",
       " 'mibsam',\n",
       " 'philistines',\n",
       " 'entered',\n",
       " 'planted',\n",
       " 'spee',\n",
       " 'dreams',\n",
       " 'anah',\n",
       " 'maidservants',\n",
       " 'perizzite',\n",
       " 'philistim',\n",
       " 'weaned',\n",
       " 'buryingplace',\n",
       " 'pluckt',\n",
       " 'samlah',\n",
       " 'tillest',\n",
       " 'gera',\n",
       " 'destroyed',\n",
       " 'isaac',\n",
       " 'rulers',\n",
       " 'arrayed',\n",
       " 'weapons',\n",
       " 'aholibamah',\n",
       " 'sheba',\n",
       " 'hezron',\n",
       " 'nahath',\n",
       " 'chaldees',\n",
       " 'fishes',\n",
       " 'troughs',\n",
       " 'gre',\n",
       " 'chedorlaomer',\n",
       " 'became',\n",
       " 'tribes',\n",
       " 'castles',\n",
       " 'canaan',\n",
       " 'elah',\n",
       " 'beasts',\n",
       " 'faces',\n",
       " 'hittites',\n",
       " 'hitti',\n",
       " 'women',\n",
       " 'onam',\n",
       " 'co',\n",
       " 'magicians',\n",
       " 'haran',\n",
       " 'divineth',\n",
       " 'dedan',\n",
       " 'egy',\n",
       " 'fowls',\n",
       " 'serah',\n",
       " 'savoury',\n",
       " 'fruits',\n",
       " 'iram',\n",
       " 'rebuked',\n",
       " 'wiv',\n",
       " 'mizpah',\n",
       " 'ajah',\n",
       " 'wagons',\n",
       " 'shiloh',\n",
       " 'ophir',\n",
       " 'jubal',\n",
       " 'phichol',\n",
       " 'breasts',\n",
       " 'skins',\n",
       " 'ruled',\n",
       " 'malchiel',\n",
       " 'captives',\n",
       " 'vessels',\n",
       " 'shechem',\n",
       " 'hou',\n",
       " 'lahairoi',\n",
       " 'visions',\n",
       " 'melchizedek',\n",
       " 'joktan',\n",
       " 'kinds',\n",
       " 'esau',\n",
       " 'bones',\n",
       " 'canst',\n",
       " 'tr',\n",
       " 'eliezer',\n",
       " 'ard',\n",
       " 'seeth',\n",
       " 'letushim',\n",
       " 'killed',\n",
       " 'sepulchres',\n",
       " 'chezib',\n",
       " 'budded',\n",
       " 'waxed',\n",
       " 'assyria',\n",
       " 'hamathite',\n",
       " 'tarried',\n",
       " 'towns',\n",
       " 'gaham',\n",
       " 'perceived',\n",
       " 'conspired',\n",
       " 'baalhanan',\n",
       " 'ishbak',\n",
       " 'dreamed',\n",
       " 'commandments',\n",
       " 'dishon',\n",
       " 'putting',\n",
       " 'concubines',\n",
       " 'dainties',\n",
       " 'asketh',\n",
       " 'shel',\n",
       " 'dunge',\n",
       " 'dinhabah',\n",
       " 'sepulchre',\n",
       " 'diklah',\n",
       " 'reviv',\n",
       " 'answered',\n",
       " 'clothed',\n",
       " 'strakes',\n",
       " 'colts',\n",
       " 'japhe',\n",
       " 'befell',\n",
       " 'bondmen',\n",
       " 'uzal',\n",
       " 'dishan',\n",
       " 'enos',\n",
       " 'laws',\n",
       " 'firstlings',\n",
       " 'knoweth',\n",
       " 'walked',\n",
       " 'rebelled',\n",
       " 'mahalaleel',\n",
       " 'goshen',\n",
       " 'ethiopia',\n",
       " 'jobab',\n",
       " 'boys',\n",
       " 'hadoram',\n",
       " 'ashbel',\n",
       " 'naphish',\n",
       " 'anoth',\n",
       " 'euphrates',\n",
       " 'enemies',\n",
       " 'benam',\n",
       " 'meeteth',\n",
       " 'hairs',\n",
       " 'badne',\n",
       " 'progenitors',\n",
       " 'erected',\n",
       " 'husham',\n",
       " 'endued',\n",
       " 'tiras',\n",
       " 'earrings',\n",
       " 'methusael',\n",
       " 'fetcht',\n",
       " 'gatam',\n",
       " 'lentiles',\n",
       " 'adbeel',\n",
       " 'shepho',\n",
       " 'hous',\n",
       " 'abelmizraim',\n",
       " 'karnaim',\n",
       " 'jegarsahadutha',\n",
       " 'jemuel',\n",
       " 'avenged',\n",
       " 'hazezontamar',\n",
       " 'presented',\n",
       " 'medan',\n",
       " 'refused',\n",
       " 'aprons',\n",
       " 'findeth',\n",
       " 'concubi',\n",
       " 'egypt',\n",
       " 'searched',\n",
       " 'daughers',\n",
       " 'heber',\n",
       " 'liest',\n",
       " 'households',\n",
       " 'lights',\n",
       " 'children',\n",
       " 'attained',\n",
       " 'sinite',\n",
       " 'simeon',\n",
       " 'guiding',\n",
       " 'intreated',\n",
       " 'beersheba',\n",
       " 'waters',\n",
       " 'fo',\n",
       " 'cherubims',\n",
       " 'mamre',\n",
       " 'wives',\n",
       " 'mishma',\n",
       " 'eshban',\n",
       " 'accad',\n",
       " 'generatio',\n",
       " 'ki',\n",
       " 'shillem',\n",
       " 'hamor',\n",
       " 'dwe',\n",
       " 'stretched',\n",
       " 'salem',\n",
       " 'ringstraked',\n",
       " 'birsha',\n",
       " 'seas',\n",
       " 'kohath',\n",
       " 'overtook',\n",
       " 'hasted',\n",
       " 'achbor',\n",
       " 'grisl',\n",
       " 'succoth',\n",
       " 'hastened',\n",
       " 'padan',\n",
       " 'deprived',\n",
       " 'ararat',\n",
       " 'manahath',\n",
       " 'euphrat',\n",
       " 'numbering',\n",
       " 'asshurim',\n",
       " 'zohar',\n",
       " 'carcases',\n",
       " 'trees',\n",
       " 'perizzit',\n",
       " 'ears',\n",
       " 'flo',\n",
       " 'admah',\n",
       " 'repented',\n",
       " 'zuzims',\n",
       " 'eyes',\n",
       " 'adam',\n",
       " 'handmaidens',\n",
       " 'leummim',\n",
       " 'amorites',\n",
       " 'hands',\n",
       " 'asher',\n",
       " 'commanded',\n",
       " 'pulled',\n",
       " 'muppim',\n",
       " 'horites',\n",
       " 'curseth',\n",
       " 'methuselah',\n",
       " 'hazarmaveth',\n",
       " 'likene',\n",
       " 'rephaims',\n",
       " 'hanged',\n",
       " 'awaked',\n",
       " 'grap',\n",
       " 'stars',\n",
       " 'yielded',\n",
       " 'shewed',\n",
       " 'hast',\n",
       " 'traffick',\n",
       " 'gavest',\n",
       " 'rul',\n",
       " 'abram',\n",
       " 'speckl',\n",
       " 'zibeon',\n",
       " 'commended',\n",
       " 'hagar',\n",
       " 'sarah',\n",
       " 'caused',\n",
       " 'compasseth',\n",
       " 'ellasar',\n",
       " 'jehovahjireh',\n",
       " 'delivered',\n",
       " 'bethuel',\n",
       " 'riphath',\n",
       " 'seekest',\n",
       " 'fainted',\n",
       " 'seasons',\n",
       " 'dothan',\n",
       " 'hid',\n",
       " 'thorns',\n",
       " 'leaped',\n",
       " 'passed',\n",
       " 'chode',\n",
       " 'asked',\n",
       " 'bilhan',\n",
       " 'zemarite',\n",
       " 'persons',\n",
       " 'abated',\n",
       " 'naphtali',\n",
       " 'zarah',\n",
       " 'hadst',\n",
       " 'jac',\n",
       " 'held',\n",
       " 'zeboiim',\n",
       " 'jidlaph',\n",
       " 'heth',\n",
       " 'korah',\n",
       " 'timna',\n",
       " 'favour',\n",
       " 'girgashites',\n",
       " 'judah',\n",
       " 'possessions',\n",
       " 'areli',\n",
       " 'seest',\n",
       " 'zeboim',\n",
       " 'jeush',\n",
       " 'serva',\n",
       " 'isa',\n",
       " 'windows',\n",
       " 'cubits',\n",
       " 'jetur',\n",
       " 'ephraim',\n",
       " 'hobah',\n",
       " 'strengthened',\n",
       " 'near',\n",
       " 'mightier',\n",
       " 'heels',\n",
       " 'builded',\n",
       " 'hang',\n",
       " 'shouldest',\n",
       " 'enoch',\n",
       " 'carmi',\n",
       " 'irad',\n",
       " 'servants',\n",
       " 'isles',\n",
       " 'goest',\n",
       " 'blessi',\n",
       " 'horsemen',\n",
       " 'bozrah',\n",
       " 'bands',\n",
       " 'obal',\n",
       " 'riv',\n",
       " 'darkne',\n",
       " 'abraham',\n",
       " 'hated',\n",
       " 'sephar',\n",
       " 'liveth',\n",
       " 'abimael',\n",
       " 'mezahab',\n",
       " 'dost',\n",
       " 'milcah',\n",
       " 'flocks',\n",
       " 'elon',\n",
       " 'leanfleshed',\n",
       " 'thistles',\n",
       " 'egyptians',\n",
       " 'reigned',\n",
       " 'elishah',\n",
       " 'deceived',\n",
       " 'naaman',\n",
       " 'jahzeel',\n",
       " 'aileth',\n",
       " 'sakes',\n",
       " 'numbered',\n",
       " 'tak',\n",
       " 'stones',\n",
       " 'restored',\n",
       " 'hul',\n",
       " 'reproved',\n",
       " 'abel',\n",
       " 'kedemah',\n",
       " 'daught',\n",
       " 'fema',\n",
       " 'phuvah',\n",
       " 'calneh',\n",
       " 'pillows',\n",
       " 'redeemed',\n",
       " 'avith',\n",
       " 'kemuel',\n",
       " 'beerlahairoi',\n",
       " 'kirjatharba',\n",
       " 'gentiles',\n",
       " 'girgasite',\n",
       " 'gr',\n",
       " 'nati',\n",
       " 'isui',\n",
       " 'bashemath',\n",
       " 'gard',\n",
       " 'padanaram',\n",
       " 'matred',\n",
       " 'pharaoh',\n",
       " 'hittite',\n",
       " 'wombs',\n",
       " 'oth',\n",
       " 'hazo',\n",
       " 'midian',\n",
       " 'tamar',\n",
       " 'asses',\n",
       " 'jebusites',\n",
       " 'thoughts',\n",
       " 'sheaves',\n",
       " 'subtil',\n",
       " 'shobal',\n",
       " 'saul',\n",
       " 'mahalath',\n",
       " 'ezbon',\n",
       " 'amraphel',\n",
       " 'departing',\n",
       " 'interpreted',\n",
       " 'bakemeats',\n",
       " 'ceased',\n",
       " 'asenath',\n",
       " 'judith',\n",
       " 'lamentati',\n",
       " 'pitched',\n",
       " 'bak',\n",
       " 'jimnah',\n",
       " 'lamech',\n",
       " 'trembled',\n",
       " 'zidon',\n",
       " 'discerned',\n",
       " 'ishmeelites',\n",
       " 'sojourned',\n",
       " 'timnah',\n",
       " 'denied',\n",
       " 'goats',\n",
       " 'dea',\n",
       " 'syrian',\n",
       " 'aner',\n",
       " 'judged',\n",
       " 'themselv',\n",
       " 'died',\n",
       " 'hith',\n",
       " 'opened',\n",
       " 'fath',\n",
       " 'kid',\n",
       " 'canaanitish',\n",
       " 'nineveh',\n",
       " 'guni',\n",
       " 'calah',\n",
       " 'methusa',\n",
       " 'gilead',\n",
       " 'jabal',\n",
       " 'doeth',\n",
       " 'prisoners',\n",
       " 'mayest',\n",
       " 'pris',\n",
       " 'gaza',\n",
       " 'almodad',\n",
       " 'slayeth',\n",
       " 'ships',\n",
       " 'camels',\n",
       " 'peniel',\n",
       " 'separated',\n",
       " 'sheleph',\n",
       " 'prayed',\n",
       " 'thahash',\n",
       " 'mountains',\n",
       " 'tim',\n",
       " 'shebah',\n",
       " 'inhabitants',\n",
       " 'issachar',\n",
       " 'borders',\n",
       " 'elders',\n",
       " 'embraced',\n",
       " 'decreased',\n",
       " 'siddim',\n",
       " 'whosoever',\n",
       " 'casluhim',\n",
       " 'fatfleshed',\n",
       " 'chariots',\n",
       " 'abrah',\n",
       " 'conceived',\n",
       " 'creepeth',\n",
       " 'gifts',\n",
       " 'edar',\n",
       " 'rebek',\n",
       " 'ou',\n",
       " 'shaveh',\n",
       " 'princes',\n",
       " 'kine',\n",
       " 'fountains',\n",
       " 'shur',\n",
       " 'ashteroth',\n",
       " 'shinab',\n",
       " 'amal',\n",
       " 'moveth',\n",
       " 'rameses',\n",
       " 'sered',\n",
       " 'wor',\n",
       " 'edom',\n",
       " 'named',\n",
       " 'arbah',\n",
       " 'coats',\n",
       " 'pursued',\n",
       " 'halted',\n",
       " 'laughed',\n",
       " 'travailed',\n",
       " 'worshipped',\n",
       " 'atad',\n",
       " 'elam',\n",
       " 'kenaz',\n",
       " 'fearest',\n",
       " 'gomorrah',\n",
       " 'omar',\n",
       " 'ammon',\n",
       " 'bera',\n",
       " 'manasseh',\n",
       " 'offerings',\n",
       " 'circumcis',\n",
       " 'fulfilled',\n",
       " 'gerar',\n",
       " 'potiphar',\n",
       " 'jerah',\n",
       " 'eliphaz',\n",
       " 'ohad',\n",
       " 'sheddeth',\n",
       " 'lotan',\n",
       " 'babel',\n",
       " 'buz',\n",
       " 'serug',\n",
       " 'silv',\n",
       " 'bered',\n",
       " 'sheweth',\n",
       " 'womenservants',\n",
       " 'loveth',\n",
       " 'counted',\n",
       " 'ephron',\n",
       " 'masrekah',\n",
       " 'haggi',\n",
       " 'rosh',\n",
       " 'madai',\n",
       " 'jetheth',\n",
       " 'leah',\n",
       " 'jahleel',\n",
       " 'visited',\n",
       " 'zebulun',\n",
       " 'caphtorim',\n",
       " 'bakers',\n",
       " 'devoured',\n",
       " 'hushim',\n",
       " 'observed',\n",
       " 'changed',\n",
       " 'kings',\n",
       " 'shew',\n",
       " 'loved',\n",
       " 'messes',\n",
       " 'laban',\n",
       " 'kenizzites',\n",
       " 'things',\n",
       " 'akan',\n",
       " 'hil',\n",
       " 'shinar',\n",
       " 'tebah',\n",
       " 'findest',\n",
       " 'sheepshearers',\n",
       " 'bethlehem',\n",
       " 'remaineth',\n",
       " 'sidon',\n",
       " 'mizraim',\n",
       " 'natio',\n",
       " 'hirah',\n",
       " 'shammah',\n",
       " 'pharez',\n",
       " 'drinketh',\n",
       " 'hemam',\n",
       " 'salah',\n",
       " 'rebekah',\n",
       " 'reub',\n",
       " 'booths',\n",
       " 'huppim',\n",
       " 'lads',\n",
       " 'noah',\n",
       " 'westwa',\n",
       " 'sitnah',\n",
       " 'parts',\n",
       " 'gutters',\n",
       " 'gro',\n",
       " 'hor',\n",
       " 'zimran',\n",
       " 'wentest',\n",
       " 'horite',\n",
       " 'erech',\n",
       " 'beeri',\n",
       " 'communed',\n",
       " 'possessi',\n",
       " 'gershon',\n",
       " 'breaketh',\n",
       " 'kids',\n",
       " 'horses',\n",
       " 'nahor',\n",
       " 'reuel',\n",
       " 'abidah',\n",
       " 'dodanim',\n",
       " 'families',\n",
       " 'zaavan',\n",
       " 'believed',\n",
       " 'hebrews',\n",
       " 'statutes',\n",
       " 'ludim',\n",
       " 'escaped',\n",
       " 'joined',\n",
       " 'mehetabel',\n",
       " 'moabites',\n",
       " 'dumah',\n",
       " 'younge',\n",
       " 'faults',\n",
       " 'looked',\n",
       " 'whoso',\n",
       " 'loins',\n",
       " 'mocked',\n",
       " 'mibzar',\n",
       " 'alvan',\n",
       " 'lovest',\n",
       " 'fathers',\n",
       " 'ste',\n",
       " 'ezer',\n",
       " 'peop',\n",
       " 'jezer',\n",
       " 'digged',\n",
       " 'bundles',\n",
       " 'sichem',\n",
       " 'hearkened',\n",
       " 'firmame',\n",
       " 'messengers',\n",
       " 'blossoms',\n",
       " 'nimrod',\n",
       " 'machir',\n",
       " 'ephra',\n",
       " 'edomites',\n",
       " 'threshingfloor',\n",
       " 'menservants',\n",
       " 'vowedst',\n",
       " 'reached',\n",
       " 'angels',\n",
       " 'spake',\n",
       " 'potipherah',\n",
       " 'eshcol',\n",
       " 'cometh',\n",
       " 'eno',\n",
       " 'hebron',\n",
       " 'lasha',\n",
       " 'tubalcain',\n",
       " 'servan',\n",
       " 'womenservan',\n",
       " 'rods',\n",
       " 'plagues',\n",
       " 'speaketh',\n",
       " 'ev',\n",
       " 'seir',\n",
       " 'elparan',\n",
       " 'tongues',\n",
       " 'nations',\n",
       " 'began',\n",
       " 'iscah',\n",
       " 'giveth',\n",
       " 'ribs',\n",
       " 'appe',\n",
       " 'hanoch',\n",
       " 'jabbok',\n",
       " 'luz',\n",
       " 'egyptia',\n",
       " 'aran',\n",
       " 'officers',\n",
       " 'lambs',\n",
       " 'giants',\n",
       " 'toucheth',\n",
       " 'nostrils',\n",
       " 'moriah',\n",
       " 'spilled',\n",
       " 'faileth',\n",
       " 'offeri',\n",
       " 'mizz',\n",
       " 'droves',\n",
       " 'levi',\n",
       " 'ziphion',\n",
       " 'waited',\n",
       " 'embalmed',\n",
       " 'years',\n",
       " 'defiledst',\n",
       " 'cheran',\n",
       " 'lifted',\n",
       " 'deeds',\n",
       " 'naphtuhim',\n",
       " 'herds',\n",
       " 'hunter',\n",
       " 'joseph',\n",
       " 'epher',\n",
       " 'jared',\n",
       " 'clusters',\n",
       " 'ishmael',\n",
       " 'signs',\n",
       " 'beari',\n",
       " 'multiplied',\n",
       " 'galeed',\n",
       " 'shuni',\n",
       " 'togeth',\n",
       " 'suffered',\n",
       " 'phallu',\n",
       " 'handfuls',\n",
       " 'sto',\n",
       " 'daughte',\n",
       " 'moab',\n",
       " 'images',\n",
       " 'hundredfo',\n",
       " 'pressed',\n",
       " 'zebul',\n",
       " 'jewels',\n",
       " 'places',\n",
       " 'overthrew',\n",
       " 'eri',\n",
       " 'jacob',\n",
       " 'zepho',\n",
       " 'arodi',\n",
       " 'hills',\n",
       " 'anointedst',\n",
       " 'played',\n",
       " 'goeth',\n",
       " 'zerah',\n",
       " 'lands',\n",
       " 'bilhah',\n",
       " 'obeyed',\n",
       " 'hori',\n",
       " 'honourable',\n",
       " 'javan',\n",
       " 'cana',\n",
       " 'remembered',\n",
       " 'cainan',\n",
       " 'mules',\n",
       " 'espied',\n",
       " 'hai',\n",
       " 'amalekites',\n",
       " 'consumed',\n",
       " 'youngest',\n",
       " 'nebajoth',\n",
       " 'favoured',\n",
       " 'catt',\n",
       " 'rachel',\n",
       " 'amorite',\n",
       " 'spies',\n",
       " 'pieces',\n",
       " 'healed',\n",
       " 'urged',\n",
       " 'hemdan',\n",
       " 'manass',\n",
       " 'arvadite',\n",
       " 'dinah',\n",
       " 'blesseth',\n",
       " 'preserved',\n",
       " 'gods',\n",
       " 'sabtech',\n",
       " 'longedst',\n",
       " 'archers',\n",
       " 'prevailed',\n",
       " 'famished',\n",
       " 'foals',\n",
       " 'hivite',\n",
       " 'lingered',\n",
       " 'thi',\n",
       " 'asswaged',\n",
       " 'sceptre',\n",
       " 'supplanted',\n",
       " 'bre',\n",
       " 'lighted',\n",
       " 'resen',\n",
       " 'fulfil',\n",
       " 'knowest',\n",
       " 'huz',\n",
       " 'israel',\n",
       " 'laded',\n",
       " 'ehi',\n",
       " 'imagined',\n",
       " 'heads',\n",
       " 'circumcised',\n",
       " 'prospered',\n",
       " 'marvelled',\n",
       " 'childr',\n",
       " 'shrubs',\n",
       " 'rained',\n",
       " 'purposing',\n",
       " 'emptied',\n",
       " 'asshur',\n",
       " 'mandrakes',\n",
       " 'verified',\n",
       " 'habitations',\n",
       " 'shuah',\n",
       " 'knees',\n",
       " 'lords',\n",
       " 'adullamite',\n",
       " 'arioch',\n",
       " 'strangers',\n",
       " 'birds',\n",
       " 'togarmah',\n",
       " 'despised',\n",
       " 'tola',\n",
       " 'thousands',\n",
       " 'mehujael',\n",
       " 'words',\n",
       " 'oversig',\n",
       " 'sinning',\n",
       " 'ephrath',\n",
       " 'adah',\n",
       " 'marriages',\n",
       " 'morever',\n",
       " 'keturah',\n",
       " 'ahuzzath',\n",
       " 'mercies',\n",
       " 'eber',\n",
       " 'bulls',\n",
       " 'spe',\n",
       " 'deborah',\n",
       " 'blessings',\n",
       " 'priests',\n",
       " 'reu',\n",
       " 'shimron',\n",
       " 'temani',\n",
       " 'moreh',\n",
       " 'isra',\n",
       " 'jaalam',\n",
       " 'stronger',\n",
       " 'honour',\n",
       " 'hadar',\n",
       " 'saith',\n",
       " 'oa',\n",
       " 'comest',\n",
       " 'jamin',\n",
       " 'comforted',\n",
       " 'ones',\n",
       " 'butlers',\n",
       " 'onan',\n",
       " 'pla',\n",
       " 'committed',\n",
       " 'raamah',\n",
       " 'compassed',\n",
       " 'begettest',\n",
       " 'walketh',\n",
       " 'anamim',\n",
       " 'refrained',\n",
       " 'arphaxad',\n",
       " 'slimepits',\n",
       " 'nuts',\n",
       " 'stories',\n",
       " 'rested',\n",
       " 'mam',\n",
       " 'baskets',\n",
       " 'cakes',\n",
       " 'instruments',\n",
       " 'feet',\n",
       " 'rams',\n",
       " 'appeared',\n",
       " 'branches',\n",
       " 'sacks',\n",
       " 'repenteth',\n",
       " 'struggled',\n",
       " 'midianites',\n",
       " ...}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_of_words(text, wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Choose your own webpage (in html format) and output the 20 most common words in this web page. You should use w.lower() to normalize the text. Please get rid of stop words, numbers, and punctuations. Please define a function and call that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = \"https://scikit-learn.org/stable/modules/linear_model.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "stopwords= nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def freq_words(url, n):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    text = BeautifulSoup(html, 'html.parser').get_text()\n",
    "    fd = nltk.FreqDist(word.lower() for word in nltk.word_tokenize(text) if(word not in stopwords) and word.isalpha())\n",
    "    return [word for (word, _) in fd.most_common(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'regression',\n",
       " 'model',\n",
       " 'linear',\n",
       " 'w',\n",
       " 'x',\n",
       " 'regularization',\n",
       " 'data',\n",
       " 'coefficients',\n",
       " 'features',\n",
       " 'least',\n",
       " 'number',\n",
       " 'lasso',\n",
       " 'ridge',\n",
       " 'models',\n",
       " 'parameter',\n",
       " 'samples',\n",
       " 'using',\n",
       " 'squares',\n",
       " 'it']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words(decomposition, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
