{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Describe the class of strings matched by the following regular expressions.No code is needed and just describe what the following regular expressions do/match).\n",
    "\n",
    "#### a.[a-zA-Z]+\n",
    "#### containing one or more alphabet letter\n",
    "\n",
    "\n",
    "#### b.[A-Z][a-z]*\n",
    "#### Containing zero or more alphabet letter\n",
    "\n",
    "\n",
    "\n",
    "#### c.p[aeiou]{,2}t\n",
    "#### Containing string one letter 'p' and any of (a,e,i,o,u ) for 0 to 2 times and one letter 't' in the string.\n",
    "\n",
    "\n",
    "#### d.\\d+(\\.\\d+)?\n",
    "### matches a digit character and zero or more number after period \n",
    "\n",
    "\n",
    "#### e.([^aeiou][aeiou][^aeiou])*\n",
    "\n",
    "#### zero or more of match of any character not in [a,e,i,o,u] and any character matches [a,e,i,o,u] and any character not in [a,e,i,o,u]\n",
    "\n",
    "\n",
    "\n",
    "#### f.\\w+(?:[-']\\w+)*|[-.(]+|\\S\\w*\n",
    "#### find words and words there are - ' between words and just find a quote or one or more character with - . ( or any white space or underscore with the word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Rewrite the following loop as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "    result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 4), ('is', 2), ('an', 2), ('introduction', 12), ('class', 5)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Read in some text from your own document in your local disk, tokenize it, and use the regular expressions to  print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) (hint: import nltk;  import re; from nltk import word_tokenize) (use lower() and set()). Please use lower() to normalize the text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root= r\"C:\\Users\\hlee0\\Desktop\\project\\CIS 9665\\assignment 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints/Assignment_2_Questions_2022(1)-checkpoint.ipynb',\n",
       " 'Assignment 2_2022(1).pdf',\n",
       " 'Assignment_2_Questions_2022(1).ipynb',\n",
       " 'email.txt',\n",
       " 'practice.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assumptions', 'for', 'linear', 'regression', '.', ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = wordlists.words('email.txt')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "whs = [w.lower() for w in tokens if re.search('^wh', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create your own file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=open('practice.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tax 55\\n',\n",
       " 'Law 34\\n',\n",
       " 'math 34\\n',\n",
       " 'tech 23\\n',\n",
       " 'brother 23\\n',\n",
       " 'avast 23\\n',\n",
       " 'google 23\\n',\n",
       " 'geforece 11\\n',\n",
       " 'logitech 32\\n',\n",
       " 'data 54\\n',\n",
       " 'discord 32\\n',\n",
       " 'om 10\\n',\n",
       " 'utilities 87\\n',\n",
       " 'prot 78']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for sub in raw:\n",
    "    res.append(sub.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tax 55',\n",
       " 'Law 34',\n",
       " 'math 34',\n",
       " 'tech 23',\n",
       " 'brother 23',\n",
       " 'avast 23',\n",
       " 'google 23',\n",
       " 'geforece 11',\n",
       " 'logitech 32',\n",
       " 'data 54',\n",
       " 'discord 32',\n",
       " 'om 10',\n",
       " 'utilities 87',\n",
       " 'prot 78']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tax', '55']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans =[]\n",
    "for e in res:\n",
    "    ans.append(e.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tax', '55'],\n",
       " ['Law', '34'],\n",
       " ['math', '34'],\n",
       " ['tech', '23'],\n",
       " ['brother', '23'],\n",
       " ['avast', '23'],\n",
       " ['google', '23'],\n",
       " ['geforece', '11'],\n",
       " ['logitech', '32'],\n",
       " ['data', '54'],\n",
       " ['discord', '32'],\n",
       " ['om', '10'],\n",
       " ['utilities', '87'],\n",
       " ['prot', '78']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 .Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for each section of the Brown Corpus (i.e. News, Editorial,…, Humor). Please use two ways introduced in the class to calculate the average number of letters per word μw and the average number of words per sentences μs. You are supposed to get two different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#μw average number of letters per word\n",
    "#μs to be the average number of words per sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average number of words per sentence in the Brown Corpus\n",
    "\n",
    "len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564780"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.raw(categories='adventure'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure 24.41\n",
      "belles_lettres 31.1\n",
      "editorial 30.03\n",
      "fiction 25.15\n",
      "government 32.88\n",
      "hobbies 29.14\n",
      "humor 28.07\n",
      "learned 31.91\n",
      "lore 30.39\n",
      "mystery 24.16\n",
      "news 30.83\n",
      "religion 30.38\n",
      "reviews 31.13\n",
      "romance 24.71\n",
      "science_fiction 25.29\n"
     ]
    }
   ],
   "source": [
    "for category in brown.categories():\n",
    "    chars=len(brown.raw(categories=category))\n",
    "    words=len(brown.words(categories=category))\n",
    "    sens=len(brown.sents(categories=category))\n",
    "    avg_let = chars/words\n",
    "    avg_wor = words/sens\n",
    "    ARI = 4.71*avg_let + 0.5*avg_wor - 21.43\n",
    "    print(category, round(ARI,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure 4.08\n",
      "belles_lettres 10.99\n",
      "editorial 9.47\n",
      "fiction 4.91\n",
      "government 12.08\n",
      "hobbies 8.92\n",
      "humor 7.89\n",
      "learned 11.93\n",
      "lore 10.25\n",
      "mystery 3.83\n",
      "news 10.18\n",
      "religion 10.2\n",
      "reviews 10.77\n",
      "romance 4.35\n",
      "science_fiction 4.98\n"
     ]
    }
   ],
   "source": [
    "for category in brown.categories():\n",
    "    words = brown.words(categories=category)\n",
    "    sents = brown.sents(categories=category)\n",
    "    avg_let = sum(len(w) for w in words)/ len(words)\n",
    "    avg_wor = sum(len(sent)for sent in sents)/len(sents)\n",
    "    ARI = 4.71*avg_let + 0.5*avg_wor - 21.43\n",
    "    print(category, round(ARI,2))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Use the Porter Stemmer to normalize some tokenized text (see below), calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and describe any difference you observe by using these two stemmers. Finally, please try Lemmatization by using WordNet Lemmatizer and describe any difference you observe compared to Porter Stemmer and Lancaster Stemmer.\n",
    "\n",
    "#### text='Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    " text='Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog',\n",
       " 'base',\n",
       " 'on',\n",
       " 'nlp',\n",
       " 'are',\n",
       " 'becom',\n",
       " 'increasingli',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'for',\n",
       " 'exampl',\n",
       " ',',\n",
       " 'phone',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'comput',\n",
       " 'support',\n",
       " 'predict',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwrit',\n",
       " 'recognit',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engin',\n",
       " 'give',\n",
       " 'access',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'lock',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstructur',\n",
       " 'text',\n",
       " ';',\n",
       " 'machin',\n",
       " 'translat',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'retriev',\n",
       " 'text',\n",
       " 'written',\n",
       " 'in',\n",
       " 'chines',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'spanish',\n",
       " ';',\n",
       " 'text',\n",
       " 'analysi',\n",
       " 'enabl',\n",
       " 'us',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'sentiment',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'by',\n",
       " 'provid',\n",
       " 'more',\n",
       " 'natur',\n",
       " 'human-machin',\n",
       " 'interfac',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'sophist',\n",
       " 'access',\n",
       " 'to',\n",
       " 'store',\n",
       " 'inform',\n",
       " ',',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'ha',\n",
       " 'come',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'central',\n",
       " 'role',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multilingu',\n",
       " 'inform',\n",
       " 'societi']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog',\n",
       " 'bas',\n",
       " 'on',\n",
       " 'nlp',\n",
       " 'ar',\n",
       " 'becom',\n",
       " 'increas',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'for',\n",
       " 'exampl',\n",
       " ',',\n",
       " 'phon',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'comput',\n",
       " 'support',\n",
       " 'predict',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwrit',\n",
       " 'recognit',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engin',\n",
       " 'giv',\n",
       " 'access',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'lock',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstruct',\n",
       " 'text',\n",
       " ';',\n",
       " 'machin',\n",
       " 'transl',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'retriev',\n",
       " 'text',\n",
       " 'writ',\n",
       " 'in',\n",
       " 'chines',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'span',\n",
       " ';',\n",
       " 'text',\n",
       " 'analys',\n",
       " 'en',\n",
       " 'us',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'senty',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'by',\n",
       " 'provid',\n",
       " 'mor',\n",
       " 'nat',\n",
       " 'human-machine',\n",
       " 'interfac',\n",
       " ',',\n",
       " 'and',\n",
       " 'mor',\n",
       " 'soph',\n",
       " 'access',\n",
       " 'to',\n",
       " 'stor',\n",
       " 'inform',\n",
       " ',',\n",
       " 'langu',\n",
       " 'process',\n",
       " 'has',\n",
       " 'com',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'cent',\n",
       " 'rol',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multil',\n",
       " 'inform',\n",
       " 'socy']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancaster stemmer is more aggressive in stemming process. For example, enable became en in lancaster but it's enabl in porter. \n",
    "#Also society has big difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Technologies',\n",
       " 'based',\n",
       " 'on',\n",
       " 'NLP',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'phone',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'computer',\n",
       " 'support',\n",
       " 'predictive',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwriting',\n",
       " 'recognition',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engine',\n",
       " 'give',\n",
       " 'access',\n",
       " 'to',\n",
       " 'information',\n",
       " 'locked',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstructured',\n",
       " 'text',\n",
       " ';',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'allows',\n",
       " 'u',\n",
       " 'to',\n",
       " 'retrieve',\n",
       " 'text',\n",
       " 'written',\n",
       " 'in',\n",
       " 'Chinese',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'Spanish',\n",
       " ';',\n",
       " 'text',\n",
       " 'analysis',\n",
       " 'enables',\n",
       " 'u',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'sentiment',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'By',\n",
       " 'providing',\n",
       " 'more',\n",
       " 'natural',\n",
       " 'human-machine',\n",
       " 'interface',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'sophisticated',\n",
       " 'access',\n",
       " 'to',\n",
       " 'stored',\n",
       " 'information',\n",
       " ',',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'ha',\n",
       " 'come',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'central',\n",
       " 'role',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multilingual',\n",
       " 'information',\n",
       " 'society']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.Rewrite the following nested loop by using a list comprehension and regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'ai',\n",
       " 'aiu',\n",
       " 'aiui',\n",
       " 'aiuio',\n",
       " 'e',\n",
       " 'ea',\n",
       " 'eai',\n",
       " 'eaio',\n",
       " 'eaiou',\n",
       " 'eo',\n",
       " 'eou',\n",
       " 'eoui',\n",
       " 'eouio',\n",
       " 'eu',\n",
       " 'euo',\n",
       " 'euoi',\n",
       " 'euoia',\n",
       " 'o',\n",
       " 'oa',\n",
       " 'oau',\n",
       " 'oaua',\n",
       " 'oauai',\n",
       " 'oauaio',\n",
       " 'u',\n",
       " 'ui',\n",
       " 'uii',\n",
       " 'uiie',\n",
       " 'uiiei',\n",
       " 'uiieio',\n",
       " 'uiieioa']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution',\n",
    "      'sequoia', 'tenacious', 'unidirectional']\n",
    "vsequences = set()\n",
    "for word in words:\n",
    "    vowels = []\n",
    "    for char in word:\n",
    "        if char in 'aeiou':\n",
    "            vowels.append(char)\n",
    "            vsequences.add(''.join(vowels))\n",
    "sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution',\n",
    "      'sequoia', 'tenacious', 'unidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.Try to refer the following sample code to print the following sentences in a formatted way.(Hint: you should use str.format() method in print() and for loop；For more information, please read the textbook section 3.9 in chapter 3) \n",
    "#### Output should exactly look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2282321570.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [19]\u001b[1;36m\u001b[0m\n\u001b[1;33m    The Tragedie of Hamlet was written by William Shakespeare in 1599\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
    "Leaves of Grass        was written by Walt Whiteman       in 1855\n",
    "Emma                   was written by Jane Austen         in 1816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n"
     ]
    }
   ],
   "source": [
    "template = 'Lee wants a {} right now'\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template.format(snack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '{} was written by {} in {}'\n",
    "title = ['The Tragedie of Hamlet','Leaves of Grass','Emma']\n",
    "authors = ['William Shakespeare', 'Walt Whiteman' , 'Jane Austen']\n",
    "years = ['1599', '1855', '1816']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary = {'title':['The Tragedie of Hamlet','Leaves of Grass','Emma'], 'author':['William Shakespeare', 'Walt Whiteman' , 'Jane Austen'], 'year':['1599', '1855', '1816']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
      "Leaves of Grass was written by Walt Whiteman in 1855\n",
      "Emma was written by Jane Austen in 1816\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for title in title:\n",
    "    author=authors[i]\n",
    "    year = years[i]\n",
    "    print(template.format(title,author,year))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Please first use sentence tokenization to print out the first 10 sentences in the \"carroll-alice.txt\" in the Gutenberg corpus. And then use basic corpus functionality sents() to return the first 10 sentences in this book. Please describe the difference between the two results.  (hint: use sent_tokenzie (), pprint(), sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.corpus.gutenberg.raw('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(sents[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "carroll=gutenberg.sents('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!',\n",
      " \"I shall be late!'\",\n",
      " '(when she thought it over afterwards, it\\n'\n",
      " 'occurred to her that she ought to have wondered at this, but at the time\\n'\n",
      " 'it all seemed quite natural); but when the Rabbit actually TOOK A WATCH\\n'\n",
      " 'OUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on,\\n'\n",
      " 'Alice started to her feet, for it flashed across her mind that she had\\n'\n",
      " 'never before seen a rabbit with either a waistcoat-pocket, or a watch\\n'\n",
      " 'to take out of it, and burning with curiosity, she ran across the field\\n'\n",
      " 'after it, and fortunately was just in time to see it pop down a large\\n'\n",
      " 'rabbit-hole under the hedge.',\n",
      " 'In another moment down went Alice after it, never once considering how\\n'\n",
      " 'in the world she was to get out again.',\n",
      " 'The rabbit-hole went straight on like a tunnel for some way, and then\\n'\n",
      " 'dipped suddenly down, so suddenly that Alice had not a moment to think\\n'\n",
      " 'about stopping herself before she found herself falling down a very deep\\n'\n",
      " 'well.',\n",
      " 'Either the well was very deep, or she fell very slowly, for she had\\n'\n",
      " 'plenty of time as she went down to look about her and to wonder what was\\n'\n",
      " 'going to happen next.']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'Alice',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'Adventures',\n",
       "  'in',\n",
       "  'Wonderland',\n",
       "  'by',\n",
       "  'Lewis',\n",
       "  'Carroll',\n",
       "  '1865',\n",
       "  ']'],\n",
       " ['CHAPTER', 'I', '.'],\n",
       " ['Down', 'the', 'Rabbit', '-', 'Hole'],\n",
       " ['Alice',\n",
       "  'was',\n",
       "  'beginning',\n",
       "  'to',\n",
       "  'get',\n",
       "  'very',\n",
       "  'tired',\n",
       "  'of',\n",
       "  'sitting',\n",
       "  'by',\n",
       "  'her',\n",
       "  'sister',\n",
       "  'on',\n",
       "  'the',\n",
       "  'bank',\n",
       "  ',',\n",
       "  'and',\n",
       "  'of',\n",
       "  'having',\n",
       "  'nothing',\n",
       "  'to',\n",
       "  'do',\n",
       "  ':',\n",
       "  'once',\n",
       "  'or',\n",
       "  'twice',\n",
       "  'she',\n",
       "  'had',\n",
       "  'peeped',\n",
       "  'into',\n",
       "  'the',\n",
       "  'book',\n",
       "  'her',\n",
       "  'sister',\n",
       "  'was',\n",
       "  'reading',\n",
       "  ',',\n",
       "  'but',\n",
       "  'it',\n",
       "  'had',\n",
       "  'no',\n",
       "  'pictures',\n",
       "  'or',\n",
       "  'conversations',\n",
       "  'in',\n",
       "  'it',\n",
       "  ',',\n",
       "  \"'\",\n",
       "  'and',\n",
       "  'what',\n",
       "  'is',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'a',\n",
       "  'book',\n",
       "  \",'\",\n",
       "  'thought',\n",
       "  'Alice',\n",
       "  \"'\",\n",
       "  'without',\n",
       "  'pictures',\n",
       "  'or',\n",
       "  'conversation',\n",
       "  \"?'\"],\n",
       " ['So',\n",
       "  'she',\n",
       "  'was',\n",
       "  'considering',\n",
       "  'in',\n",
       "  'her',\n",
       "  'own',\n",
       "  'mind',\n",
       "  '(',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'she',\n",
       "  'could',\n",
       "  ',',\n",
       "  'for',\n",
       "  'the',\n",
       "  'hot',\n",
       "  'day',\n",
       "  'made',\n",
       "  'her',\n",
       "  'feel',\n",
       "  'very',\n",
       "  'sleepy',\n",
       "  'and',\n",
       "  'stupid',\n",
       "  '),',\n",
       "  'whether',\n",
       "  'the',\n",
       "  'pleasure',\n",
       "  'of',\n",
       "  'making',\n",
       "  'a',\n",
       "  'daisy',\n",
       "  '-',\n",
       "  'chain',\n",
       "  'would',\n",
       "  'be',\n",
       "  'worth',\n",
       "  'the',\n",
       "  'trouble',\n",
       "  'of',\n",
       "  'getting',\n",
       "  'up',\n",
       "  'and',\n",
       "  'picking',\n",
       "  'the',\n",
       "  'daisies',\n",
       "  ',',\n",
       "  'when',\n",
       "  'suddenly',\n",
       "  'a',\n",
       "  'White',\n",
       "  'Rabbit',\n",
       "  'with',\n",
       "  'pink',\n",
       "  'eyes',\n",
       "  'ran',\n",
       "  'close',\n",
       "  'by',\n",
       "  'her',\n",
       "  '.'],\n",
       " ['There',\n",
       "  'was',\n",
       "  'nothing',\n",
       "  'so',\n",
       "  'VERY',\n",
       "  'remarkable',\n",
       "  'in',\n",
       "  'that',\n",
       "  ';',\n",
       "  'nor',\n",
       "  'did',\n",
       "  'Alice',\n",
       "  'think',\n",
       "  'it',\n",
       "  'so',\n",
       "  'VERY',\n",
       "  'much',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'way',\n",
       "  'to',\n",
       "  'hear',\n",
       "  'the',\n",
       "  'Rabbit',\n",
       "  'say',\n",
       "  'to',\n",
       "  'itself',\n",
       "  ',',\n",
       "  \"'\",\n",
       "  'Oh',\n",
       "  'dear',\n",
       "  '!'],\n",
       " ['Oh', 'dear', '!'],\n",
       " ['I', 'shall', 'be', 'late', \"!'\"],\n",
       " ['(',\n",
       "  'when',\n",
       "  'she',\n",
       "  'thought',\n",
       "  'it',\n",
       "  'over',\n",
       "  'afterwards',\n",
       "  ',',\n",
       "  'it',\n",
       "  'occurred',\n",
       "  'to',\n",
       "  'her',\n",
       "  'that',\n",
       "  'she',\n",
       "  'ought',\n",
       "  'to',\n",
       "  'have',\n",
       "  'wondered',\n",
       "  'at',\n",
       "  'this',\n",
       "  ',',\n",
       "  'but',\n",
       "  'at',\n",
       "  'the',\n",
       "  'time',\n",
       "  'it',\n",
       "  'all',\n",
       "  'seemed',\n",
       "  'quite',\n",
       "  'natural',\n",
       "  ');',\n",
       "  'but',\n",
       "  'when',\n",
       "  'the',\n",
       "  'Rabbit',\n",
       "  'actually',\n",
       "  'TOOK',\n",
       "  'A',\n",
       "  'WATCH',\n",
       "  'OUT',\n",
       "  'OF',\n",
       "  'ITS',\n",
       "  'WAISTCOAT',\n",
       "  '-',\n",
       "  'POCKET',\n",
       "  ',',\n",
       "  'and',\n",
       "  'looked',\n",
       "  'at',\n",
       "  'it',\n",
       "  ',',\n",
       "  'and',\n",
       "  'then',\n",
       "  'hurried',\n",
       "  'on',\n",
       "  ',',\n",
       "  'Alice',\n",
       "  'started',\n",
       "  'to',\n",
       "  'her',\n",
       "  'feet',\n",
       "  ',',\n",
       "  'for',\n",
       "  'it',\n",
       "  'flashed',\n",
       "  'across',\n",
       "  'her',\n",
       "  'mind',\n",
       "  'that',\n",
       "  'she',\n",
       "  'had',\n",
       "  'never',\n",
       "  'before',\n",
       "  'seen',\n",
       "  'a',\n",
       "  'rabbit',\n",
       "  'with',\n",
       "  'either',\n",
       "  'a',\n",
       "  'waistcoat',\n",
       "  '-',\n",
       "  'pocket',\n",
       "  ',',\n",
       "  'or',\n",
       "  'a',\n",
       "  'watch',\n",
       "  'to',\n",
       "  'take',\n",
       "  'out',\n",
       "  'of',\n",
       "  'it',\n",
       "  ',',\n",
       "  'and',\n",
       "  'burning',\n",
       "  'with',\n",
       "  'curiosity',\n",
       "  ',',\n",
       "  'she',\n",
       "  'ran',\n",
       "  'across',\n",
       "  'the',\n",
       "  'field',\n",
       "  'after',\n",
       "  'it',\n",
       "  ',',\n",
       "  'and',\n",
       "  'fortunately',\n",
       "  'was',\n",
       "  'just',\n",
       "  'in',\n",
       "  'time',\n",
       "  'to',\n",
       "  'see',\n",
       "  'it',\n",
       "  'pop',\n",
       "  'down',\n",
       "  'a',\n",
       "  'large',\n",
       "  'rabbit',\n",
       "  '-',\n",
       "  'hole',\n",
       "  'under',\n",
       "  'the',\n",
       "  'hedge',\n",
       "  '.'],\n",
       " ['In',\n",
       "  'another',\n",
       "  'moment',\n",
       "  'down',\n",
       "  'went',\n",
       "  'Alice',\n",
       "  'after',\n",
       "  'it',\n",
       "  ',',\n",
       "  'never',\n",
       "  'once',\n",
       "  'considering',\n",
       "  'how',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'she',\n",
       "  'was',\n",
       "  'to',\n",
       "  'get',\n",
       "  'out',\n",
       "  'again',\n",
       "  '.']]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carroll[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way it captures sentence is different, pprint returns nontype whereas sents function returns list. \n",
    "#Also, sents count Chpater 1 as sentence where as pprint insert in with before sentence "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
